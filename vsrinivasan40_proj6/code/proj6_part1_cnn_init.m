function net = proj6_part1_cnn_init(methodology_num)
%code for Computer Vision, Georgia Tech by James Hays
%based of the MNIST example from MatConvNet

rng('default');
rng(0);

% constant scalar for the random initial network weights. You shouldn't
% need to modify this.
f=1/100; 

net.layers = {} ;

if(methodology_num==1 || methodology_num==2 || methodology_num==3)
    net.layers{end+1} = struct('type', 'conv', ...
        'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
        'stride', 1, ...
        'pad', 0, ...
        'name', 'conv1') ;

    net.layers{end+1} = struct('type', 'pool', ...
        'method', 'max', ...
        'pool', [7 7], ...
        'stride', 7, ...
        'pad', 0) ;

    net.layers{end+1} = struct('type', 'relu') ;

    net.layers{end+1} = struct('type', 'conv', ...
        'weights', {{f*randn(8,8,10,15, 'single'), zeros(1, 15, 'single')}}, ...
        'stride', 1, ...
        'pad', 0, ...
        'name', 'fc1') ;

    % Loss layer
    net.layers{end+1} = struct('type', 'softmaxloss') ;

elseif(methodology_num==4)

    net.layers{end+1} = struct('type', 'conv', ...
        'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
        'stride', 1, ...
        'pad', 0, ...
        'name', 'conv1') ;

    net.layers{end+1} = struct('type', 'pool', ...
        'method', 'max', ...
        'pool', [7 7], ...
        'stride', 7, ...
        'pad', 0) ;

    net.layers{end+1} = struct('type', 'relu') ;
    
    net.layers{end+1} = struct('type', 'dropout', ...
        'rate', 0.5) ;

    net.layers{end+1} = struct('type', 'conv', ...
        'weights', {{f*randn(8,8,10,15, 'single'), zeros(1, 15, 'single')}}, ...
        'stride', 1, ...
        'pad', 0, ...
        'name', 'fc1') ;

    % Loss layer
    net.layers{end+1} = struct('type', 'softmaxloss') ;
    
elseif(methodology_num==5)
    net.layers{end+1} = struct('type', 'conv', ...
                               'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
                               'stride', 1, ...
                               'pad', 0, ...
                               'name', 'conv1') ;

    net.layers{end+1} = struct('type', 'pool', ...
                               'method', 'max', ...
                               'pool', [3 3], ... % was [5 5]
                               'stride', 2, ...
                               'pad', 0) ;

    net.layers{end+1} = struct('type', 'relu') ;


    net.layers{end+1} = struct('type', 'conv', ...
                               'weights', {{f*randn(5,5,10,10, 'single'), zeros(1, 10, 'single')}}, ...
                               'stride', 1, ...
                               'pad', 0, ...
                               'name', 'conv2') ;

    net.layers{end+1} = struct('type', 'pool', ...
                               'method', 'max', ...
                               'pool', [3 3], ...
                               'stride', 2, ...
                               'pad', 0) ;

    net.layers{end+1} = struct('type','dropout', ...
                                'rate',0.5) ;

    net.layers{end+1} = struct('type', 'conv', ... % used to be 10,10 instead of 11,11
                               'weights', {{f*randn(11,11,10,15, 'single'), zeros(1, 15, 'single')}}, ...
                               'stride', 1, ...
                               'pad', 0, ...
                               'name', 'fc1') ;
                           
    % Loss layer
    net.layers{end+1} = struct('type', 'softmaxloss') ;
end


% Visualize the network
vl_simplenn_display(net, 'inputSize', [64 64 1 50])